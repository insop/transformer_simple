{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVM, load transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/isong/Downloads/ml/sc/xilinx/Github/transformer_simple/src/python\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/isong/anaconda3x/envs/nlu/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "OPTIONS  Namespace(batch_size=4, debug=True, depth=1, embedding_size=128, final=False, gradient_clipping=1.0, lr=0.0001, lr_warmup=10000, max_length=512, max_pool=False, model_name='single_transformer.pt', num_epochs=1, num_heads=1, seed=1, tb_dir='./runs', tiny=True, vocab_size=50000)\n",
      "/Users/isong/anaconda3x/envs/nlu/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
      "/Users/isong/anaconda3x/envs/nlu/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "- nr. of training examples 63\n",
      "- nr. of validation examples 63\n",
      "Model's state_dict:\n",
      "token_embedding.weight \t torch.Size([50000, 128])\n",
      "pos_embedding.weight \t torch.Size([512, 128])\n",
      "trfm_blocks.0.mha.attentions.0.toqueries.weight \t torch.Size([128, 128])\n",
      "trfm_blocks.0.mha.attentions.0.toqueries.bias \t torch.Size([128])\n",
      "trfm_blocks.0.mha.attentions.0.tokeys.weight \t torch.Size([128, 128])\n",
      "trfm_blocks.0.mha.attentions.0.tokeys.bias \t torch.Size([128])\n",
      "trfm_blocks.0.mha.attentions.0.tovalues.weight \t torch.Size([128, 128])\n",
      "trfm_blocks.0.mha.attentions.0.tovalues.bias \t torch.Size([128])\n",
      "trfm_blocks.0.mha.w_o.0.weight \t torch.Size([128, 128])\n",
      "trfm_blocks.0.mha.w_o.0.bias \t torch.Size([128])\n",
      "trfm_blocks.0.norm1.weight \t torch.Size([128])\n",
      "trfm_blocks.0.norm1.bias \t torch.Size([128])\n",
      "trfm_blocks.0.norm2.weight \t torch.Size([128])\n",
      "trfm_blocks.0.norm2.bias \t torch.Size([128])\n",
      "trfm_blocks.0.ff.0.weight \t torch.Size([512, 128])\n",
      "trfm_blocks.0.ff.0.bias \t torch.Size([512])\n",
      "trfm_blocks.0.ff.2.weight \t torch.Size([128, 512])\n",
      "trfm_blocks.0.ff.2.bias \t torch.Size([128])\n",
      "toprobs.weight \t torch.Size([2, 128])\n",
      "toprobs.bias \t torch.Size([2])\n",
      "token_embedding.weight \t tensor([[-1.2033e+00, -1.0272e-01,  4.8644e-01,  ...,  8.4201e-01,\n",
      "          1.9471e-02,  1.2484e+00],\n",
      "        [ 7.8966e-01, -4.4316e-01, -2.0543e+00,  ...,  3.7122e-01,\n",
      "         -2.8077e-01,  7.0841e-01],\n",
      "        [-8.9393e-01,  1.9724e-01,  4.7868e-01,  ..., -1.0859e+00,\n",
      "          4.2945e-01,  5.0907e-01],\n",
      "        ...,\n",
      "        [ 7.3678e-04, -6.3191e-01,  3.0112e-01,  ...,  1.8285e-01,\n",
      "         -8.9899e-01,  9.3205e-01],\n",
      "        [ 1.3843e+00,  5.3569e-01, -2.6828e-01,  ..., -1.1396e-02,\n",
      "         -2.6063e+00,  9.1748e-01],\n",
      "        [-1.2996e+00, -5.2440e-01, -1.5959e+00,  ...,  3.0580e-01,\n",
      "          1.1791e+00, -9.2770e-01]])\n",
      "pos_embedding.weight \t tensor([[ 0.0523,  0.4881,  0.1075,  ..., -0.8936,  2.1964, -0.5453],\n",
      "        [-0.5146, -0.2876,  0.4577,  ..., -1.7958, -1.1295,  1.1521],\n",
      "        [-0.9597, -0.0504,  0.7354,  ...,  1.2932,  2.9757,  1.8266],\n",
      "        ...,\n",
      "        [ 0.0049,  2.0542, -1.6063,  ..., -0.0973,  0.7059,  1.4313],\n",
      "        [ 1.1547,  0.5256,  0.9073,  ...,  2.2664,  0.1552,  0.0306],\n",
      "        [-0.7071, -1.1530,  0.3818,  ..., -0.5725, -0.3261, -0.7989]])\n",
      "trfm_blocks.0.mha.attentions.0.toqueries.weight \t tensor([[ 0.0425,  0.0633, -0.0203,  ..., -0.0037, -0.0660, -0.0314],\n",
      "        [-0.0068, -0.0585,  0.0577,  ...,  0.0881, -0.0192, -0.0659],\n",
      "        [ 0.0491,  0.0192, -0.0243,  ..., -0.0787, -0.0196,  0.0556],\n",
      "        ...,\n",
      "        [ 0.0668, -0.0257,  0.0155,  ...,  0.0487,  0.0876, -0.0454],\n",
      "        [ 0.0578, -0.0438,  0.0104,  ...,  0.0425, -0.0420,  0.0866],\n",
      "        [ 0.0383, -0.0817,  0.0514,  ..., -0.0063,  0.0227,  0.0256]])\n",
      "trfm_blocks.0.mha.attentions.0.toqueries.bias \t tensor([-0.0880, -0.0650,  0.0089,  0.0221, -0.0494, -0.0187,  0.0181,  0.0653,\n",
      "         0.0615, -0.0751,  0.0686, -0.0405,  0.0272, -0.0444,  0.0607,  0.0628,\n",
      "         0.0343,  0.0582, -0.0502, -0.0275, -0.0405, -0.0492, -0.0244, -0.0631,\n",
      "        -0.0797, -0.0837, -0.0080, -0.0755, -0.0038,  0.0546, -0.0144, -0.0136,\n",
      "        -0.0068,  0.0067, -0.0685, -0.0004, -0.0147, -0.0058, -0.0140, -0.0764,\n",
      "        -0.0836,  0.0015, -0.0286, -0.0682, -0.0310,  0.0040, -0.0531,  0.0170,\n",
      "        -0.0581, -0.0696,  0.0474,  0.0030, -0.0390, -0.0329,  0.0627, -0.0651,\n",
      "        -0.0315, -0.0093,  0.0304, -0.0382, -0.0492,  0.0714, -0.0249,  0.0058,\n",
      "        -0.0794,  0.0642, -0.0640, -0.0401,  0.0102, -0.0269, -0.0520,  0.0561,\n",
      "        -0.0078,  0.0131, -0.0703, -0.0295,  0.0742, -0.0636,  0.0224,  0.0480,\n",
      "         0.0373,  0.0308, -0.0176, -0.0453,  0.0848, -0.0762,  0.0155, -0.0094,\n",
      "         0.0369,  0.0358, -0.0439,  0.0662, -0.0757,  0.0466, -0.0394, -0.0776,\n",
      "        -0.0128,  0.0441, -0.0279, -0.0863,  0.0438, -0.0390,  0.0266,  0.0812,\n",
      "        -0.0471,  0.0869,  0.0534, -0.0536, -0.0383,  0.0365, -0.0549,  0.0413,\n",
      "         0.0864, -0.0409,  0.0606,  0.0397,  0.0622,  0.0654,  0.0584,  0.0296,\n",
      "         0.0803,  0.0439,  0.0229,  0.0297,  0.0775, -0.0678, -0.0074, -0.0775])\n",
      "trfm_blocks.0.mha.attentions.0.tokeys.weight \t tensor([[ 0.0875, -0.0182,  0.0077,  ..., -0.0046, -0.0017, -0.0704],\n",
      "        [-0.0271, -0.0215,  0.0481,  ...,  0.0487, -0.0068, -0.0045],\n",
      "        [-0.0767, -0.0644, -0.0869,  ..., -0.0084,  0.0653,  0.0553],\n",
      "        ...,\n",
      "        [-0.0218,  0.0675, -0.0860,  ...,  0.0593, -0.0799, -0.0848],\n",
      "        [-0.0076, -0.0010, -0.0622,  ...,  0.0718, -0.0493,  0.0865],\n",
      "        [-0.0851, -0.0805,  0.0233,  ..., -0.0427, -0.0304, -0.0683]])\n",
      "trfm_blocks.0.mha.attentions.0.tokeys.bias \t tensor([ 0.0683, -0.0235, -0.0198,  0.0287, -0.0132,  0.0727,  0.0190, -0.0772,\n",
      "         0.0084, -0.0211, -0.0437,  0.0777, -0.0311,  0.0454, -0.0210, -0.0189,\n",
      "        -0.0598, -0.0666,  0.0613, -0.0321,  0.0701, -0.0564, -0.0515,  0.0803,\n",
      "         0.0310, -0.0047, -0.0346, -0.0192, -0.0434,  0.0708,  0.0521,  0.0088,\n",
      "         0.0517, -0.0172,  0.0003, -0.0564, -0.0714,  0.0640,  0.0847, -0.0279,\n",
      "        -0.0087,  0.0763,  0.0845,  0.0170, -0.0692,  0.0600, -0.0145, -0.0737,\n",
      "        -0.0332,  0.0845,  0.0044,  0.0318,  0.0349, -0.0046, -0.0195,  0.0683,\n",
      "         0.0620,  0.0176, -0.0868,  0.0056,  0.0607,  0.0188, -0.0077,  0.0045,\n",
      "        -0.0475, -0.0596, -0.0543,  0.0665,  0.0250,  0.0433, -0.0062, -0.0066,\n",
      "         0.0068,  0.0509,  0.0738,  0.0491, -0.0629, -0.0691,  0.0167,  0.0295,\n",
      "         0.0527, -0.0018,  0.0626, -0.0053, -0.0453,  0.0089,  0.0379, -0.0524,\n",
      "         0.0288,  0.0854,  0.0068, -0.0863, -0.0848, -0.0800,  0.0025,  0.0587,\n",
      "        -0.0275, -0.0723, -0.0348, -0.0232, -0.0642,  0.0162,  0.0471,  0.0520,\n",
      "        -0.0231, -0.0082,  0.0609,  0.0212,  0.0651, -0.0041,  0.0149,  0.0835,\n",
      "        -0.0846, -0.0138,  0.0244,  0.0807, -0.0174,  0.0524, -0.0514, -0.0587,\n",
      "        -0.0536, -0.0478,  0.0873,  0.0514, -0.0109, -0.0552,  0.0043, -0.0078])\n",
      "trfm_blocks.0.mha.attentions.0.tovalues.weight \t tensor([[-0.0570,  0.0327,  0.0464,  ...,  0.0622,  0.0126, -0.0680],\n",
      "        [ 0.0321, -0.0222,  0.0485,  ...,  0.0198,  0.0414, -0.0796],\n",
      "        [ 0.0544,  0.0039, -0.0240,  ..., -0.0650, -0.0375,  0.0605],\n",
      "        ...,\n",
      "        [-0.0054,  0.0349,  0.0859,  ...,  0.0365,  0.0668,  0.0614],\n",
      "        [ 0.0205,  0.0846, -0.0494,  ...,  0.0854,  0.0779, -0.0882],\n",
      "        [ 0.0247,  0.0850,  0.0058,  ..., -0.0685, -0.0587, -0.0833]])\n",
      "trfm_blocks.0.mha.attentions.0.tovalues.bias \t tensor([ 0.0490, -0.0786,  0.0204,  0.0453,  0.0730,  0.0143,  0.0381, -0.0268,\n",
      "        -0.0610, -0.0064, -0.0567,  0.0035, -0.0662,  0.0619,  0.0432, -0.0418,\n",
      "        -0.0601, -0.0287,  0.0294, -0.0037, -0.0352,  0.0814,  0.0554, -0.0482,\n",
      "        -0.0713, -0.0710,  0.0783, -0.0240,  0.0814,  0.0276, -0.0720, -0.0698,\n",
      "        -0.0579,  0.0338,  0.0213,  0.0646, -0.0056,  0.0294,  0.0039, -0.0016,\n",
      "        -0.0087,  0.0146, -0.0426,  0.0411,  0.0228,  0.0115,  0.0596, -0.0531,\n",
      "         0.0699,  0.0168,  0.0873,  0.0781, -0.0052, -0.0270, -0.0776, -0.0250,\n",
      "        -0.0838, -0.0144,  0.0371, -0.0269,  0.0022, -0.0873,  0.0667, -0.0667,\n",
      "        -0.0506, -0.0390,  0.0760,  0.0595, -0.0148, -0.0466, -0.0246,  0.0675,\n",
      "        -0.0788, -0.0469,  0.0080,  0.0174, -0.0549, -0.0238, -0.0846, -0.0472,\n",
      "         0.0082, -0.0795,  0.0706, -0.0684,  0.0396, -0.0288,  0.0852,  0.0461,\n",
      "        -0.0663,  0.0876,  0.0195, -0.0742, -0.0774, -0.0306, -0.0832, -0.0409,\n",
      "        -0.0142, -0.0408,  0.0217,  0.0428, -0.0325, -0.0282, -0.0763, -0.0559,\n",
      "         0.0494,  0.0298, -0.0867,  0.0759,  0.0062,  0.0646, -0.0486, -0.0506,\n",
      "        -0.0038, -0.0412, -0.0082, -0.0633, -0.0324, -0.0008,  0.0388,  0.0785,\n",
      "        -0.0522,  0.0109,  0.0044, -0.0439,  0.0041, -0.0555, -0.0729,  0.0595])\n",
      "trfm_blocks.0.mha.w_o.0.weight \t tensor([[-0.0060,  0.0275,  0.0303,  ...,  0.0775, -0.0092,  0.0804],\n",
      "        [-0.0112,  0.0237, -0.0881,  ..., -0.0448, -0.0872, -0.0719],\n",
      "        [ 0.0134,  0.0070,  0.0542,  ..., -0.0651, -0.0550, -0.0007],\n",
      "        ...,\n",
      "        [-0.0772, -0.0699, -0.0741,  ...,  0.0846, -0.0235,  0.0443],\n",
      "        [-0.0337, -0.0636, -0.0307,  ..., -0.0788, -0.0630,  0.0190],\n",
      "        [-0.0683,  0.0751, -0.0333,  ...,  0.0808,  0.0050,  0.0407]])\n",
      "trfm_blocks.0.mha.w_o.0.bias \t tensor([ 0.0748, -0.0223,  0.0288,  0.0268, -0.0126,  0.0626,  0.0550, -0.0452,\n",
      "        -0.0816,  0.0095, -0.0021,  0.0320,  0.0378, -0.0099, -0.0068,  0.0623,\n",
      "         0.0463,  0.0059, -0.0185, -0.0799, -0.0479, -0.0332, -0.0869,  0.0869,\n",
      "        -0.0882,  0.0703, -0.0375,  0.0116, -0.0573, -0.0494, -0.0117, -0.0487,\n",
      "         0.0052, -0.0718, -0.0724,  0.0554, -0.0239, -0.0566, -0.0213,  0.0641,\n",
      "        -0.0737,  0.0234,  0.0026,  0.0809, -0.0414, -0.0066, -0.0577, -0.0285,\n",
      "         0.0144, -0.0242, -0.0806, -0.0118, -0.0221, -0.0166,  0.0757, -0.0305,\n",
      "         0.0247, -0.0706, -0.0530,  0.0572, -0.0823, -0.0626, -0.0033, -0.0159,\n",
      "        -0.0871,  0.0558,  0.0802,  0.0803,  0.0558,  0.0298,  0.0282, -0.0155,\n",
      "        -0.0123, -0.0522,  0.0444, -0.0413,  0.0452,  0.0828,  0.0012, -0.0143,\n",
      "        -0.0293,  0.0432,  0.0021,  0.0690,  0.0457,  0.0849,  0.0413,  0.0203,\n",
      "         0.0454,  0.0844,  0.0244, -0.0041,  0.0093,  0.0238, -0.0590,  0.0740,\n",
      "         0.0675, -0.0055,  0.0441,  0.0220, -0.0217,  0.0316, -0.0085,  0.0220,\n",
      "        -0.0569,  0.0279,  0.0817,  0.0465,  0.0225, -0.0381, -0.0174, -0.0091,\n",
      "        -0.0421,  0.0320,  0.0787, -0.0190, -0.0399, -0.0099,  0.0540, -0.0474,\n",
      "         0.0618,  0.0226, -0.0654, -0.0717, -0.0340, -0.0339, -0.0477,  0.0875])\n",
      "trfm_blocks.0.norm1.weight \t tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "trfm_blocks.0.norm1.bias \t tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "trfm_blocks.0.norm2.weight \t tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "trfm_blocks.0.norm2.bias \t tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "trfm_blocks.0.ff.0.weight \t tensor([[ 0.0537,  0.0112, -0.0478,  ..., -0.0807, -0.0579, -0.0404],\n",
      "        [-0.0300,  0.0543,  0.0772,  ..., -0.0872,  0.0516,  0.0340],\n",
      "        [ 0.0019, -0.0105, -0.0451,  ...,  0.0159,  0.0587,  0.0051],\n",
      "        ...,\n",
      "        [ 0.0268,  0.0680,  0.0499,  ...,  0.0274,  0.0033,  0.0583],\n",
      "        [ 0.0170, -0.0275,  0.0718,  ...,  0.0575,  0.0114,  0.0941],\n",
      "        [ 0.0670,  0.0270,  0.0297,  ...,  0.0933, -0.0164,  0.0371]])\n",
      "trfm_blocks.0.ff.0.bias \t tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "trfm_blocks.0.ff.2.weight \t tensor([[ 0.0014,  0.0690,  0.0383,  ..., -0.0059, -0.0146,  0.0679],\n",
      "        [-0.0899,  0.0567, -0.0585,  ..., -0.0586, -0.0279, -0.0647],\n",
      "        [ 0.0190,  0.0699, -0.0387,  ...,  0.0930,  0.0083, -0.0518],\n",
      "        ...,\n",
      "        [-0.0837, -0.0682, -0.0690,  ...,  0.0324, -0.0118, -0.0302],\n",
      "        [-0.0196,  0.0427, -0.0928,  ..., -0.0271, -0.0253, -0.0003],\n",
      "        [ 0.0942, -0.0188, -0.0736,  ..., -0.0656, -0.0332, -0.0796]])\n",
      "trfm_blocks.0.ff.2.bias \t tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "toprobs.weight \t tensor([[ 0.0146,  0.0738,  0.0212,  0.0531, -0.0270,  0.0226,  0.0469, -0.0599,\n",
      "          0.0729,  0.0680, -0.0753, -0.0259, -0.0454,  0.0188,  0.0314,  0.0399,\n",
      "          0.0761, -0.0774,  0.0009, -0.0056,  0.0592,  0.0763,  0.0831,  0.0880,\n",
      "          0.0056, -0.0511,  0.0745, -0.0853, -0.0207,  0.0712, -0.0067, -0.0352,\n",
      "          0.0864, -0.0601, -0.0155,  0.0306,  0.0077,  0.0206,  0.0592, -0.0176,\n",
      "          0.0761,  0.0276, -0.0514, -0.0629,  0.0216, -0.0164,  0.0856, -0.0868,\n",
      "         -0.0388,  0.0059,  0.0714,  0.0057, -0.0073,  0.0684,  0.0180,  0.0694,\n",
      "          0.0208,  0.0581, -0.0131,  0.0261, -0.0278,  0.0394, -0.0053,  0.0200,\n",
      "         -0.0123,  0.0157,  0.0161,  0.0432,  0.0606, -0.0659,  0.0149, -0.0220,\n",
      "         -0.0667, -0.0669, -0.0647,  0.0027,  0.0172, -0.0195,  0.0442, -0.0871,\n",
      "         -0.0299,  0.0207, -0.0114,  0.0525,  0.0751, -0.0052,  0.0492,  0.0451,\n",
      "         -0.0093,  0.0139, -0.0235,  0.0720,  0.0049,  0.0353, -0.0065, -0.0823,\n",
      "         -0.0285,  0.0159,  0.0758,  0.0007,  0.0195,  0.0796,  0.0300, -0.0033,\n",
      "         -0.0227,  0.0868, -0.0232,  0.0774, -0.0712,  0.0646, -0.0395,  0.0841,\n",
      "         -0.0698, -0.0641, -0.0316, -0.0051,  0.0100, -0.0752, -0.0327,  0.0333,\n",
      "         -0.0347, -0.0548,  0.0636,  0.0289, -0.0678,  0.0695,  0.0601, -0.0042],\n",
      "        [-0.0247,  0.0482,  0.0608,  0.0519,  0.0592,  0.0230, -0.0693, -0.0391,\n",
      "          0.0007,  0.0671, -0.0210, -0.0124,  0.0097, -0.0339,  0.0500,  0.0253,\n",
      "         -0.0846,  0.0782, -0.0318, -0.0742,  0.0620,  0.0570,  0.0339,  0.0052,\n",
      "          0.0359,  0.0064,  0.0583, -0.0621, -0.0165, -0.0577,  0.0166, -0.0571,\n",
      "          0.0500,  0.0782, -0.0474, -0.0021, -0.0200,  0.0561,  0.0229, -0.0519,\n",
      "         -0.0446,  0.0806,  0.0329, -0.0425, -0.0307, -0.0726, -0.0533, -0.0657,\n",
      "         -0.0371,  0.0531,  0.0051, -0.0859,  0.0600, -0.0306,  0.0729,  0.0065,\n",
      "          0.0178, -0.0684, -0.0293, -0.0139, -0.0493,  0.0129, -0.0724, -0.0659,\n",
      "          0.0263,  0.0658, -0.0872, -0.0670, -0.0694, -0.0116,  0.0358,  0.0332,\n",
      "          0.0275, -0.0461,  0.0612,  0.0726, -0.0381,  0.0279,  0.0115,  0.0778,\n",
      "         -0.0362, -0.0196,  0.0206, -0.0665, -0.0527,  0.0229, -0.0199, -0.0152,\n",
      "         -0.0229, -0.0373, -0.0038,  0.0755, -0.0233, -0.0774, -0.0021, -0.0722,\n",
      "         -0.0476,  0.0588, -0.0497,  0.0422,  0.0363, -0.0730,  0.0626,  0.0878,\n",
      "         -0.0460,  0.0705,  0.0251, -0.0527, -0.0260,  0.0535,  0.0318,  0.0212,\n",
      "         -0.0458, -0.0863,  0.0788,  0.0486,  0.0407, -0.0643, -0.0154,  0.0258,\n",
      "         -0.0508, -0.0661, -0.0080,  0.0171,  0.0150, -0.0201, -0.0595, -0.0173]])\n",
      "toprobs.bias \t tensor([0.0219, 0.0626])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.0, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'initial_lr': 0.0001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]}]\n",
      "\n",
      " epoch 0\n",
      "  0%|                                                    | 0/63 [00:00<?, ?it/s]/Users/isong/anaconda3x/envs/nlu/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 63/63 [00:04<00:00, 13.18it/s]\n",
      "-- validation accuracy 0.504\n",
      "Save model to saved_model/single_transformer.pt\n",
      "==============\n",
      "Load model to saved_model/single_transformer1.pt\n",
      "output_test\n",
      "tensor([[-0.7327, -0.6551],\n",
      "        [-0.7253, -0.6620],\n",
      "        [-0.7330, -0.6548],\n",
      "        [-0.7324, -0.6554]], grad_fn=<LogSoftmaxBackward>)\n",
      "output_load\n",
      "tensor([[-0.7327, -0.6551],\n",
      "        [-0.7253, -0.6620],\n",
      "        [-0.7330, -0.6548],\n",
      "        [-0.7324, -0.6554]])\n"
     ]
    }
   ],
   "source": [
    "!python ./experiments/classify.py -e 1 -t -d1 -H1 -D -m single_transformer.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pytorch model to tvm \n",
    "- [tvm reference](https://tvm.apache.org/docs/tutorials/frontend/from_pytorch.html#sphx-glr-tutorials-frontend-from-pytorch-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tvm modules\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tvm.contrib.download import download_testdata\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "import tvm\n",
    "from tvm import te\n",
    "from tvm import rpc, autotvm, relay\n",
    "from tvm.contrib import graph_runtime, download\n",
    "from tvm.contrib.debugger import debug_runtime\n",
    "from tvm.relay import transform\n",
    "from tvm import relay\n",
    "\n",
    "import vta\n",
    "from vta.testing import simulator\n",
    "from vta.top import graph_pack\n",
    "\n",
    "# Make sure that TVM was compiled with RPC=1\n",
    "assert tvm.runtime.enabled(\"rpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer modules\n",
    "\n",
    "import transformer_simple\n",
    "import classifier\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"transformer\"\n",
    "# single transformer\n",
    "mx = 512\n",
    "embedding_size = 128\n",
    "vocab_size = 50000\n",
    "NUM_CLS = 2\n",
    "max_pool = False\n",
    "num_heads = 1\n",
    "depth = 1\n",
    "\n",
    "PATH = 'saved_model/single_transformer1.pt'\n",
    "\n",
    "model = classifier.TransformerSimpleClassify(n_seq=mx, dim_emb=embedding_size, dim_internal=embedding_size, \\\n",
    "                                                         num_tokens=vocab_size, num_classes=NUM_CLS, max_pool=max_pool, \\\n",
    "                                                         heads=num_heads, depth=depth)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isong/Downloads/ml/sc/xilinx/Github/transformer_simple/src/python/transformer_simple.py:53: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert e == self.dim_emb, f'Input embedding ({e}) should match the layer embedding ({self.dim_emb})'\n"
     ]
    }
   ],
   "source": [
    "# We grab the TorchScripted model via tracing\n",
    "input_shape = [4, 498]\n",
    "input_data = torch.randint(0, vocab_size, input_shape)\n",
    "scripted_model = torch.jit.trace(model, input_data).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the graph to Relay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = \"input0\"\n",
    "shape_list = [(input_name, input_shape)]\n",
    "mod, params = relay.frontend.from_pytorch(scripted_model, shape_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VTA testing\n",
    "from https://tvm.apache.org/docs/vta/tutorials/frontend/deploy_classification.html#sphx-glr-vta-tutorials-frontend-deploy-classification-py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading VTA parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = vta.get_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the platform and model targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VTA parameters from the 3rdparty/vta-hw/config/vta_config.json file\n",
    "env = vta.get_env()\n",
    "\n",
    "# Set ``device=arm_cpu`` to run inference on the CPU\n",
    "# or ``device=vta`` to run inference on the FPGA.\n",
    "device = \"vta\"\n",
    "target = env.target if device == \"vta\" else env.target_vta_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPGA programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env.TARGET not in [\"sim\", \"tsim\"]:\n",
    "\n",
    "    # Get remote from tracker node if environment variable is set.\n",
    "    # To set up the tracker, you'll need to follow the \"Auto-tuning\n",
    "    # a convolutional network for VTA\" tutorial.\n",
    "    tracker_host = os.environ.get(\"TVM_TRACKER_HOST\", None)\n",
    "    tracker_port = os.environ.get(\"TVM_TRACKER_PORT\", None)\n",
    "    # Otherwise if you have a device you want to program directly from\n",
    "    # the host, make sure you've set the variables below to the IP of\n",
    "    # your board.\n",
    "    device_host = os.environ.get(\"VTA_RPC_HOST\", \"192.168.2.99\")\n",
    "    device_port = os.environ.get(\"VTA_RPC_PORT\", \"9091\")\n",
    "    if not tracker_host or not tracker_port:\n",
    "        remote = rpc.connect(device_host, int(device_port))\n",
    "    else:\n",
    "        remote = autotvm.measure.request_remote(\n",
    "            env.TARGET, tracker_host, int(tracker_port), timeout=10000\n",
    "        )\n",
    "\n",
    "    # Reconfigure the JIT runtime and FPGA.\n",
    "    # You can program the FPGA with your own custom bitstream\n",
    "    # by passing the path to the bitstream file instead of None.\n",
    "    reconfig_start = time.time()\n",
    "    vta.reconfig_runtime(remote)\n",
    "    vta.program_fpga(remote, bitstream=None)\n",
    "    reconfig_time = time.time() - reconfig_start\n",
    "    print(\"Reconfigured FPGA and RPC runtime in {0:.2f}s!\".format(reconfig_time))\n",
    "\n",
    "# In simulation mode, host the RPC server locally.\n",
    "else:\n",
    "    remote = rpc.LocalSession()\n",
    "\n",
    "# Get execution context from remote\n",
    "ctx = remote.ext_dev(0) if device == \"vta\" else remote.cpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_vta = tvm.te.placeholder(input_shape, name=\"input\", dtype=env.acc_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ext_dev -keys=vta,cpu -device=vta -model=sim_1x16_i8w8a32_15_15_18_17"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build the inference graph runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shape_dict = {}\n",
    "dtype_dict = {}\n",
    "shape_dict.update({k: v.shape for k, v in params.items()})\n",
    "dtype_dict.update({k: str(v.dtype) for k, v in params.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toprobs.bias': (2,),\n",
       " 'toprobs.weight': (2, 128),\n",
       " 'trfm_blocks.0.norm2.bias': (128,),\n",
       " 'trfm_blocks.0.norm2.weight': (128,),\n",
       " 'trfm_blocks.0.ff.2.bias': (128,),\n",
       " 'trfm_blocks.0.ff.2.weight': (128, 512),\n",
       " 'trfm_blocks.0.ff.0.bias': (512,),\n",
       " 'trfm_blocks.0.ff.0.weight': (512, 128),\n",
       " 'trfm_blocks.0.norm1.bias': (128,),\n",
       " 'trfm_blocks.0.norm1.weight': (128,),\n",
       " 'trfm_blocks.0.mha.w_o.0.bias': (128,),\n",
       " 'trfm_blocks.0.mha.w_o.0.weight': (128, 128),\n",
       " 'trfm_blocks.0.mha.attentions.0.tovalues.bias': (128,),\n",
       " 'trfm_blocks.0.mha.attentions.0.tovalues.weight': (128, 128),\n",
       " 'trfm_blocks.0.mha.attentions.0.tokeys.bias': (128,),\n",
       " 'trfm_blocks.0.mha.attentions.0.tokeys.weight': (128, 128),\n",
       " 'trfm_blocks.0.mha.attentions.0.toqueries.bias': (128,),\n",
       " 'trfm_blocks.0.mha.attentions.0.toqueries.weight': (128, 128),\n",
       " 'pos_embedding.weight': (512, 128),\n",
       " 'token_embedding.weight': (50000, 128)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toprobs.bias': 'float32',\n",
       " 'toprobs.weight': 'float32',\n",
       " 'trfm_blocks.0.norm2.bias': 'float32',\n",
       " 'trfm_blocks.0.norm2.weight': 'float32',\n",
       " 'trfm_blocks.0.ff.2.bias': 'float32',\n",
       " 'trfm_blocks.0.ff.2.weight': 'float32',\n",
       " 'trfm_blocks.0.ff.0.bias': 'float32',\n",
       " 'trfm_blocks.0.ff.0.weight': 'float32',\n",
       " 'trfm_blocks.0.norm1.bias': 'float32',\n",
       " 'trfm_blocks.0.norm1.weight': 'float32',\n",
       " 'trfm_blocks.0.mha.w_o.0.bias': 'float32',\n",
       " 'trfm_blocks.0.mha.w_o.0.weight': 'float32',\n",
       " 'trfm_blocks.0.mha.attentions.0.tovalues.bias': 'float32',\n",
       " 'trfm_blocks.0.mha.attentions.0.tovalues.weight': 'float32',\n",
       " 'trfm_blocks.0.mha.attentions.0.tokeys.bias': 'float32',\n",
       " 'trfm_blocks.0.mha.attentions.0.tokeys.weight': 'float32',\n",
       " 'trfm_blocks.0.mha.attentions.0.toqueries.bias': 'float32',\n",
       " 'trfm_blocks.0.mha.attentions.0.toqueries.weight': 'float32',\n",
       " 'pos_embedding.weight': 'float32',\n",
       " 'token_embedding.weight': 'float32'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionNode([Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), Var(input0, ty=TensorType([4, 498], int64)), Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), Var(trfm_blocks.0.mha.attentions.0.toqueries.weight, ty=TensorType([128, 128], float32)), Var(trfm_blocks.0.mha.attentions.0.toqueries.bias, ty=TensorType([128], float32)), Var(trfm_blocks.0.mha.attentions.0.tokeys.weight, ty=TensorType([128, 128], float32)), Var(trfm_blocks.0.mha.attentions.0.tokeys.bias, ty=TensorType([128], float32)), Var(trfm_blocks.0.mha.attentions.0.tovalues.weight, ty=TensorType([128, 128], float32)), Var(trfm_blocks.0.mha.attentions.0.tovalues.bias, ty=TensorType([128], float32)), Var(trfm_blocks.0.mha.w_o.0.weight, ty=TensorType([128, 128], float32)), Var(trfm_blocks.0.mha.w_o.0.bias, ty=TensorType([128], float32)), Var(trfm_blocks.0.norm1.weight, ty=TensorType([128], float32)), Var(trfm_blocks.0.norm1.bias, ty=TensorType([128], float32)), Var(trfm_blocks.0.ff.0.weight, ty=TensorType([512, 128], float32)), Var(trfm_blocks.0.ff.0.bias, ty=TensorType([512], float32)), Var(trfm_blocks.0.ff.2.weight, ty=TensorType([128, 512], float32)), Var(trfm_blocks.0.ff.2.bias, ty=TensorType([128], float32)), Var(trfm_blocks.0.norm2.weight, ty=TensorType([128], float32)), Var(trfm_blocks.0.norm2.bias, ty=TensorType([128], float32)), Var(toprobs.weight, ty=TensorType([2, 128], float32)), Var(toprobs.bias, ty=TensorType([2], float32))], TensorType([4, 2], float32), CallNode(Op(nn.log_softmax), [CallNode(Op(add), [CallNode(Op(nn.dense), [CallNode(Op(mean), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(nn.layer_norm), [CallNode(Op(add), [CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [CallNode(Op(nn.relu), [CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(nn.layer_norm), [CallNode(Op(add), [CallNode(Op(add), [CallNode(Op(zeros_like), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], (nullptr), [TensorType([4, 498, 128], float32)]), CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [CallNode(Op(nn.softmax), [CallNode(Op(divide), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], relay.attrs.ReshapeAttrs(0x7f84c7f45ac8), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.mha.attentions.0.toqueries.weight, ty=TensorType([128, 128], float32))], relay.attrs.TransposeAttrs(0x7f84f7cf8fa8), [TensorType([128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84c7f10348), [TensorType([128, 128], float32)])], relay.attrs.InitOpAttrs(0x7f84f7cfcf98), [TensorType([1, 128, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84f7cebab8), [TensorType([4, 128, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84f7cfd558), [TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.mha.attentions.0.toqueries.bias, ty=TensorType([128], float32))], (nullptr), [TensorType([4, 498, 128], float32), TensorType([128], float32)])], relay.attrs.ReshapeAttrs(0x7f84c7c84378), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(reshape), [CallNode(Op(transpose), [CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], relay.attrs.ReshapeAttrs(0x7f84f7cff1a8), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.mha.attentions.0.tokeys.weight, ty=TensorType([128, 128], float32))], relay.attrs.TransposeAttrs(0x7f84f7cfa6c8), [TensorType([128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84f7cfd858), [TensorType([128, 128], float32)])], relay.attrs.InitOpAttrs(0x7f84a7ce45f8), [TensorType([1, 128, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7cd1268), [TensorType([4, 128, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7ce4858), [TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.mha.attentions.0.tokeys.bias, ty=TensorType([128], float32))], (nullptr), [TensorType([4, 498, 128], float32), TensorType([128], float32)])], relay.attrs.TransposeAttrs(0x7f84d0fd9e08), [TensorType([4, 498, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84c7c99f38), [TensorType([4, 128, 498], float32)])], relay.attrs.TransposeAttrs(0x7f84c7c844e8), [TensorType([4, 128, 498], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84c7cf5958), [TensorType([4, 498, 498], float32)]), Constant(11.313708)], (nullptr), [TensorType([4, 498, 498], float32), TensorType([], float32)])], relay.attrs.SoftmaxAttrs(0x7f84c7c99778), [TensorType([4, 498, 498], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7c64bd8), [TensorType([4, 498, 498], float32)]), CallNode(Op(transpose), [CallNode(Op(reshape), [CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], relay.attrs.ReshapeAttrs(0x7f8507cd9358), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.mha.attentions.0.tovalues.weight, ty=TensorType([128, 128], float32))], relay.attrs.TransposeAttrs(0x7f84c7f30d98), [TensorType([128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f8507cd8d28), [TensorType([128, 128], float32)])], relay.attrs.InitOpAttrs(0x7f84a7cab788), [TensorType([1, 128, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7c67618), [TensorType([4, 128, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7cbf658), [TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.mha.attentions.0.tovalues.bias, ty=TensorType([128], float32))], (nullptr), [TensorType([4, 498, 128], float32), TensorType([128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7c68668), [TensorType([4, 498, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7ce6668), [TensorType([4, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 498], float32), TensorType([4, 128, 498], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7ce9ad8), [TensorType([4, 498, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7c8e818), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.mha.w_o.0.weight, ty=TensorType([128, 128], float32))], relay.attrs.TransposeAttrs(0x7f84a7c7ed28), [TensorType([128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7ca2c78), [TensorType([128, 128], float32)])], relay.attrs.InitOpAttrs(0x7f84a7ce2598), [TensorType([1, 128, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7ca7708), [TensorType([4, 128, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7c9bf48), [TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.mha.w_o.0.bias, ty=TensorType([128], float32))], (nullptr), [TensorType([4, 498, 128], float32), TensorType([128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)]), TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.norm1.weight, ty=TensorType([128], float32)), Var(trfm_blocks.0.norm1.bias, ty=TensorType([128], float32))], relay.attrs.LayerNormAttrs(0x7f84a7cc9c28), [TensorType([4, 498, 128], float32), TensorType([128], float32), TensorType([128], float32)])], relay.attrs.DropoutAttrs(0x7f84a7c6bf58), [TensorType([4, 498, 128], float32)]), 0)], relay.attrs.ReshapeAttrs(0x7f8487d94508), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.ff.0.weight, ty=TensorType([512, 128], float32))], relay.attrs.TransposeAttrs(0x7f84c7f14298), [TensorType([512, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f8487dc9508), [TensorType([128, 512], float32)])], relay.attrs.InitOpAttrs(0x7f8547c95688), [TensorType([1, 128, 512], float32)])], relay.attrs.TransposeAttrs(0x7f8547cb2fc8), [TensorType([4, 128, 512], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 512, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f8547cfce58), [TensorType([4, 498, 512], float32)]), Var(trfm_blocks.0.ff.0.bias, ty=TensorType([512], float32))], (nullptr), [TensorType([4, 498, 512], float32), TensorType([512], float32)])], (nullptr), [TensorType([4, 498, 512], float32)])], relay.attrs.ReshapeAttrs(0x7f84d0ffbce8), [TensorType([4, 498, 512], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.ff.2.weight, ty=TensorType([128, 512], float32))], relay.attrs.TransposeAttrs(0x7f8547cb7688), [TensorType([128, 512], float32)])], relay.attrs.ReshapeAttrs(0x7f84d0ffb8f8), [TensorType([512, 128], float32)])], relay.attrs.InitOpAttrs(0x7f84a7cc0288), [TensorType([1, 512, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7c5f9e8), [TensorType([4, 512, 128], float32)])], (nullptr), [TensorType([4, 498, 512], float32), TensorType([4, 128, 512], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7cbddc8), [TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.ff.2.bias, ty=TensorType([128], float32))], (nullptr), [TensorType([4, 498, 128], float32), TensorType([128], float32)]), TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(nn.layer_norm), [CallNode(Op(add), [CallNode(Op(add), [CallNode(Op(zeros_like), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], (nullptr), [TensorType([4, 498, 128], float32)]), CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [CallNode(Op(nn.softmax), [CallNode(Op(divide), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], relay.attrs.ReshapeAttrs(0x7f84c7f45ac8), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.mha.attentions.0.toqueries.weight, ty=TensorType([128, 128], float32))], relay.attrs.TransposeAttrs(0x7f84f7cf8fa8), [TensorType([128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84c7f10348), [TensorType([128, 128], float32)])], relay.attrs.InitOpAttrs(0x7f84f7cfcf98), [TensorType([1, 128, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84f7cebab8), [TensorType([4, 128, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84f7cfd558), [TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.mha.attentions.0.toqueries.bias, ty=TensorType([128], float32))], (nullptr), [TensorType([4, 498, 128], float32), TensorType([128], float32)])], relay.attrs.ReshapeAttrs(0x7f84c7c84378), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(reshape), [CallNode(Op(transpose), [CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], relay.attrs.ReshapeAttrs(0x7f84f7cff1a8), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.mha.attentions.0.tokeys.weight, ty=TensorType([128, 128], float32))], relay.attrs.TransposeAttrs(0x7f84f7cfa6c8), [TensorType([128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84f7cfd858), [TensorType([128, 128], float32)])], relay.attrs.InitOpAttrs(0x7f84a7ce45f8), [TensorType([1, 128, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7cd1268), [TensorType([4, 128, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7ce4858), [TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.mha.attentions.0.tokeys.bias, ty=TensorType([128], float32))], (nullptr), [TensorType([4, 498, 128], float32), TensorType([128], float32)])], relay.attrs.TransposeAttrs(0x7f84d0fd9e08), [TensorType([4, 498, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84c7c99f38), [TensorType([4, 128, 498], float32)])], relay.attrs.TransposeAttrs(0x7f84c7c844e8), [TensorType([4, 128, 498], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84c7cf5958), [TensorType([4, 498, 498], float32)]), Constant(11.313708)], (nullptr), [TensorType([4, 498, 498], float32), TensorType([], float32)])], relay.attrs.SoftmaxAttrs(0x7f84c7c99778), [TensorType([4, 498, 498], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7c64bd8), [TensorType([4, 498, 498], float32)]), CallNode(Op(transpose), [CallNode(Op(reshape), [CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], relay.attrs.ReshapeAttrs(0x7f8507cd9358), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.mha.attentions.0.tovalues.weight, ty=TensorType([128, 128], float32))], relay.attrs.TransposeAttrs(0x7f84c7f30d98), [TensorType([128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f8507cd8d28), [TensorType([128, 128], float32)])], relay.attrs.InitOpAttrs(0x7f84a7cab788), [TensorType([1, 128, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7c67618), [TensorType([4, 128, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7cbf658), [TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.mha.attentions.0.tovalues.bias, ty=TensorType([128], float32))], (nullptr), [TensorType([4, 498, 128], float32), TensorType([128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7c68668), [TensorType([4, 498, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7ce6668), [TensorType([4, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 498], float32), TensorType([4, 128, 498], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7ce9ad8), [TensorType([4, 498, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7c8e818), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.mha.w_o.0.weight, ty=TensorType([128, 128], float32))], relay.attrs.TransposeAttrs(0x7f84a7c7ed28), [TensorType([128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7ca2c78), [TensorType([128, 128], float32)])], relay.attrs.InitOpAttrs(0x7f84a7ce2598), [TensorType([1, 128, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7ca7708), [TensorType([4, 128, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7c9bf48), [TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.mha.w_o.0.bias, ty=TensorType([128], float32))], (nullptr), [TensorType([4, 498, 128], float32), TensorType([128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)]), TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.norm1.weight, ty=TensorType([128], float32)), Var(trfm_blocks.0.norm1.bias, ty=TensorType([128], float32))], relay.attrs.LayerNormAttrs(0x7f84a7cc9c28), [TensorType([4, 498, 128], float32), TensorType([128], float32), TensorType([128], float32)])], relay.attrs.DropoutAttrs(0x7f84a7c6bf58), [TensorType([4, 498, 128], float32)]), 0)], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.norm2.weight, ty=TensorType([128], float32)), Var(trfm_blocks.0.norm2.bias, ty=TensorType([128], float32))], relay.attrs.LayerNormAttrs(0x7f84a7c63b58), [TensorType([4, 498, 128], float32), TensorType([128], float32), TensorType([128], float32)])], relay.attrs.DropoutAttrs(0x7f84a7c632e8), [TensorType([4, 498, 128], float32)]), 0)], relay.attrs.ReduceAttrs(0x7f84a7c86398), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(transpose), [Var(toprobs.weight, ty=TensorType([2, 128], float32))], relay.attrs.TransposeAttrs(0x7f84a7cf0968), [TensorType([2, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7ce6998), [TensorType([128, 2], float32)])], relay.attrs.DenseAttrs(0x7f84a7ca5e98), [TensorType([4, 128], float32), TensorType([2, 128], float32)]), Var(toprobs.bias, ty=TensorType([2], float32))], (nullptr), [TensorType([4, 2], float32), TensorType([2], float32)])], relay.attrs.SoftmaxAttrs(0x7f84a7ce6d78), [TensorType([4, 2], float32)]), [], (nullptr))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod[\"main\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IRModuleNode( {GlobalVar(main): FunctionNode([Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), Var(input0, ty=TensorType([4, 498], int64)), Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), Var(trfm_blocks.0.mha.attentions.0.toqueries.weight, ty=TensorType([128, 128], float32)), Var(trfm_blocks.0.mha.attentions.0.toqueries.bias, ty=TensorType([128], float32)), Var(trfm_blocks.0.mha.attentions.0.tokeys.weight, ty=TensorType([128, 128], float32)), Var(trfm_blocks.0.mha.attentions.0.tokeys.bias, ty=TensorType([128], float32)), Var(trfm_blocks.0.mha.attentions.0.tovalues.weight, ty=TensorType([128, 128], float32)), Var(trfm_blocks.0.mha.attentions.0.tovalues.bias, ty=TensorType([128], float32)), Var(trfm_blocks.0.mha.w_o.0.weight, ty=TensorType([128, 128], float32)), Var(trfm_blocks.0.mha.w_o.0.bias, ty=TensorType([128], float32)), Var(trfm_blocks.0.norm1.weight, ty=TensorType([128], float32)), Var(trfm_blocks.0.norm1.bias, ty=TensorType([128], float32)), Var(trfm_blocks.0.ff.0.weight, ty=TensorType([512, 128], float32)), Var(trfm_blocks.0.ff.0.bias, ty=TensorType([512], float32)), Var(trfm_blocks.0.ff.2.weight, ty=TensorType([128, 512], float32)), Var(trfm_blocks.0.ff.2.bias, ty=TensorType([128], float32)), Var(trfm_blocks.0.norm2.weight, ty=TensorType([128], float32)), Var(trfm_blocks.0.norm2.bias, ty=TensorType([128], float32)), Var(toprobs.weight, ty=TensorType([2, 128], float32)), Var(toprobs.bias, ty=TensorType([2], float32))], TensorType([4, 2], float32), CallNode(Op(nn.log_softmax), [CallNode(Op(add), [CallNode(Op(nn.dense), [CallNode(Op(mean), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(nn.layer_norm), [CallNode(Op(add), [CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [CallNode(Op(nn.relu), [CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(nn.layer_norm), [CallNode(Op(add), [CallNode(Op(add), [CallNode(Op(zeros_like), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], (nullptr), [TensorType([4, 498, 128], float32)]), CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [CallNode(Op(nn.softmax), [CallNode(Op(divide), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], relay.attrs.ReshapeAttrs(0x7f84c7f45ac8), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.mha.attentions.0.toqueries.weight, ty=TensorType([128, 128], float32))], relay.attrs.TransposeAttrs(0x7f84f7cf8fa8), [TensorType([128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84c7f10348), [TensorType([128, 128], float32)])], relay.attrs.InitOpAttrs(0x7f84f7cfcf98), [TensorType([1, 128, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84f7cebab8), [TensorType([4, 128, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84f7cfd558), [TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.mha.attentions.0.toqueries.bias, ty=TensorType([128], float32))], (nullptr), [TensorType([4, 498, 128], float32), TensorType([128], float32)])], relay.attrs.ReshapeAttrs(0x7f84c7c84378), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(reshape), [CallNode(Op(transpose), [CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], relay.attrs.ReshapeAttrs(0x7f84f7cff1a8), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.mha.attentions.0.tokeys.weight, ty=TensorType([128, 128], float32))], relay.attrs.TransposeAttrs(0x7f84f7cfa6c8), [TensorType([128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84f7cfd858), [TensorType([128, 128], float32)])], relay.attrs.InitOpAttrs(0x7f84a7ce45f8), [TensorType([1, 128, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7cd1268), [TensorType([4, 128, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7ce4858), [TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.mha.attentions.0.tokeys.bias, ty=TensorType([128], float32))], (nullptr), [TensorType([4, 498, 128], float32), TensorType([128], float32)])], relay.attrs.TransposeAttrs(0x7f84d0fd9e08), [TensorType([4, 498, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84c7c99f38), [TensorType([4, 128, 498], float32)])], relay.attrs.TransposeAttrs(0x7f84c7c844e8), [TensorType([4, 128, 498], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84c7cf5958), [TensorType([4, 498, 498], float32)]), Constant(11.313708)], (nullptr), [TensorType([4, 498, 498], float32), TensorType([], float32)])], relay.attrs.SoftmaxAttrs(0x7f84c7c99778), [TensorType([4, 498, 498], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7c64bd8), [TensorType([4, 498, 498], float32)]), CallNode(Op(transpose), [CallNode(Op(reshape), [CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], relay.attrs.ReshapeAttrs(0x7f8507cd9358), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.mha.attentions.0.tovalues.weight, ty=TensorType([128, 128], float32))], relay.attrs.TransposeAttrs(0x7f84c7f30d98), [TensorType([128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f8507cd8d28), [TensorType([128, 128], float32)])], relay.attrs.InitOpAttrs(0x7f84a7cab788), [TensorType([1, 128, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7c67618), [TensorType([4, 128, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7cbf658), [TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.mha.attentions.0.tovalues.bias, ty=TensorType([128], float32))], (nullptr), [TensorType([4, 498, 128], float32), TensorType([128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7c68668), [TensorType([4, 498, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7ce6668), [TensorType([4, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 498], float32), TensorType([4, 128, 498], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7ce9ad8), [TensorType([4, 498, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7c8e818), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.mha.w_o.0.weight, ty=TensorType([128, 128], float32))], relay.attrs.TransposeAttrs(0x7f84a7c7ed28), [TensorType([128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7ca2c78), [TensorType([128, 128], float32)])], relay.attrs.InitOpAttrs(0x7f84a7ce2598), [TensorType([1, 128, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7ca7708), [TensorType([4, 128, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7c9bf48), [TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.mha.w_o.0.bias, ty=TensorType([128], float32))], (nullptr), [TensorType([4, 498, 128], float32), TensorType([128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)]), TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.norm1.weight, ty=TensorType([128], float32)), Var(trfm_blocks.0.norm1.bias, ty=TensorType([128], float32))], relay.attrs.LayerNormAttrs(0x7f84a7cc9c28), [TensorType([4, 498, 128], float32), TensorType([128], float32), TensorType([128], float32)])], relay.attrs.DropoutAttrs(0x7f84a7c6bf58), [TensorType([4, 498, 128], float32)]), 0)], relay.attrs.ReshapeAttrs(0x7f8487d94508), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.ff.0.weight, ty=TensorType([512, 128], float32))], relay.attrs.TransposeAttrs(0x7f84c7f14298), [TensorType([512, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f8487dc9508), [TensorType([128, 512], float32)])], relay.attrs.InitOpAttrs(0x7f8547c95688), [TensorType([1, 128, 512], float32)])], relay.attrs.TransposeAttrs(0x7f8547cb2fc8), [TensorType([4, 128, 512], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 512, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f8547cfce58), [TensorType([4, 498, 512], float32)]), Var(trfm_blocks.0.ff.0.bias, ty=TensorType([512], float32))], (nullptr), [TensorType([4, 498, 512], float32), TensorType([512], float32)])], (nullptr), [TensorType([4, 498, 512], float32)])], relay.attrs.ReshapeAttrs(0x7f84d0ffbce8), [TensorType([4, 498, 512], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.ff.2.weight, ty=TensorType([128, 512], float32))], relay.attrs.TransposeAttrs(0x7f8547cb7688), [TensorType([128, 512], float32)])], relay.attrs.ReshapeAttrs(0x7f84d0ffb8f8), [TensorType([512, 128], float32)])], relay.attrs.InitOpAttrs(0x7f84a7cc0288), [TensorType([1, 512, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7c5f9e8), [TensorType([4, 512, 128], float32)])], (nullptr), [TensorType([4, 498, 512], float32), TensorType([4, 128, 512], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7cbddc8), [TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.ff.2.bias, ty=TensorType([128], float32))], (nullptr), [TensorType([4, 498, 128], float32), TensorType([128], float32)]), TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(nn.layer_norm), [CallNode(Op(add), [CallNode(Op(add), [CallNode(Op(zeros_like), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], (nullptr), [TensorType([4, 498, 128], float32)]), CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [CallNode(Op(nn.softmax), [CallNode(Op(divide), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], relay.attrs.ReshapeAttrs(0x7f84c7f45ac8), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.mha.attentions.0.toqueries.weight, ty=TensorType([128, 128], float32))], relay.attrs.TransposeAttrs(0x7f84f7cf8fa8), [TensorType([128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84c7f10348), [TensorType([128, 128], float32)])], relay.attrs.InitOpAttrs(0x7f84f7cfcf98), [TensorType([1, 128, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84f7cebab8), [TensorType([4, 128, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84f7cfd558), [TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.mha.attentions.0.toqueries.bias, ty=TensorType([128], float32))], (nullptr), [TensorType([4, 498, 128], float32), TensorType([128], float32)])], relay.attrs.ReshapeAttrs(0x7f84c7c84378), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(reshape), [CallNode(Op(transpose), [CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], relay.attrs.ReshapeAttrs(0x7f84f7cff1a8), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.mha.attentions.0.tokeys.weight, ty=TensorType([128, 128], float32))], relay.attrs.TransposeAttrs(0x7f84f7cfa6c8), [TensorType([128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84f7cfd858), [TensorType([128, 128], float32)])], relay.attrs.InitOpAttrs(0x7f84a7ce45f8), [TensorType([1, 128, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7cd1268), [TensorType([4, 128, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7ce4858), [TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.mha.attentions.0.tokeys.bias, ty=TensorType([128], float32))], (nullptr), [TensorType([4, 498, 128], float32), TensorType([128], float32)])], relay.attrs.TransposeAttrs(0x7f84d0fd9e08), [TensorType([4, 498, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84c7c99f38), [TensorType([4, 128, 498], float32)])], relay.attrs.TransposeAttrs(0x7f84c7c844e8), [TensorType([4, 128, 498], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84c7cf5958), [TensorType([4, 498, 498], float32)]), Constant(11.313708)], (nullptr), [TensorType([4, 498, 498], float32), TensorType([], float32)])], relay.attrs.SoftmaxAttrs(0x7f84c7c99778), [TensorType([4, 498, 498], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7c64bd8), [TensorType([4, 498, 498], float32)]), CallNode(Op(transpose), [CallNode(Op(reshape), [CallNode(Op(add), [CallNode(Op(reshape), [CallNode(Op(nn.batch_matmul), [CallNode(Op(reshape), [TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], relay.attrs.ReshapeAttrs(0x7f8507cd9358), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.mha.attentions.0.tovalues.weight, ty=TensorType([128, 128], float32))], relay.attrs.TransposeAttrs(0x7f84c7f30d98), [TensorType([128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f8507cd8d28), [TensorType([128, 128], float32)])], relay.attrs.InitOpAttrs(0x7f84a7cab788), [TensorType([1, 128, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7c67618), [TensorType([4, 128, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7cbf658), [TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.mha.attentions.0.tovalues.bias, ty=TensorType([128], float32))], (nullptr), [TensorType([4, 498, 128], float32), TensorType([128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7c68668), [TensorType([4, 498, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7ce6668), [TensorType([4, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 498], float32), TensorType([4, 128, 498], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7ce9ad8), [TensorType([4, 498, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7c8e818), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(broadcast_to), [CallNode(Op(reshape), [CallNode(Op(transpose), [Var(trfm_blocks.0.mha.w_o.0.weight, ty=TensorType([128, 128], float32))], relay.attrs.TransposeAttrs(0x7f84a7c7ed28), [TensorType([128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7ca2c78), [TensorType([128, 128], float32)])], relay.attrs.InitOpAttrs(0x7f84a7ce2598), [TensorType([1, 128, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7ca7708), [TensorType([4, 128, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 128, 128], float32)])], relay.attrs.ReshapeAttrs(0x7f84a7c9bf48), [TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.mha.w_o.0.bias, ty=TensorType([128], float32))], (nullptr), [TensorType([4, 498, 128], float32), TensorType([128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)]), TupleGetItemNode(CallNode(Op(nn.dropout), [CallNode(Op(add), [CallNode(Op(take), [Var(token_embedding.weight, ty=TensorType([50000, 128], float32)), CallNode(Op(cast), [Var(input0, ty=TensorType([4, 498], int64))], relay.attrs.CastAttrs(0x7f8547ca0538), [TensorType([4, 498], int64)])], relay.attrs.TakeAttrs(0x7f8547c9fe48), [TensorType([50000, 128], float32), TensorType([4, 498], int32)]), CallNode(Op(repeat), [CallNode(Op(strided_slice), [CallNode(Op(strided_slice), [CallNode(Op(expand_dims), [CallNode(Op(take), [Var(pos_embedding.weight, ty=TensorType([512, 128], float32)), CallNode(Op(cast), [CallNode(Op(arange), [Constant(0), Constant(498), Constant(1)], relay.attrs.ArangeAttrs(0x7f8547cae198), [TensorType([], int64), TensorType([], int64), TensorType([], int64)])], relay.attrs.CastAttrs(0x7f8547caea68), [TensorType([498], int64)])], relay.attrs.TakeAttrs(0x7f8547c94498), [TensorType([512, 128], float32), TensorType([498], int32)])], relay.attrs.ExpandDimsAttrs(0x7f8547caebd8), [TensorType([498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cae318), [TensorType([1, 498, 128], float32)])], relay.attrs.StridedSliceAttrs(0x7f8547cb2358), [TensorType([1, 498, 128], float32)])], relay.attrs.RepeatAttrs(0x7f8547cb6ba8), [TensorType([1, 498, 128], float32)])], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)])], relay.attrs.DropoutAttrs(0x7f8547cc1408), [TensorType([4, 498, 128], float32)]), 0)], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.norm1.weight, ty=TensorType([128], float32)), Var(trfm_blocks.0.norm1.bias, ty=TensorType([128], float32))], relay.attrs.LayerNormAttrs(0x7f84a7cc9c28), [TensorType([4, 498, 128], float32), TensorType([128], float32), TensorType([128], float32)])], relay.attrs.DropoutAttrs(0x7f84a7c6bf58), [TensorType([4, 498, 128], float32)]), 0)], (nullptr), [TensorType([4, 498, 128], float32), TensorType([4, 498, 128], float32)]), Var(trfm_blocks.0.norm2.weight, ty=TensorType([128], float32)), Var(trfm_blocks.0.norm2.bias, ty=TensorType([128], float32))], relay.attrs.LayerNormAttrs(0x7f84a7c63b58), [TensorType([4, 498, 128], float32), TensorType([128], float32), TensorType([128], float32)])], relay.attrs.DropoutAttrs(0x7f84a7c632e8), [TensorType([4, 498, 128], float32)]), 0)], relay.attrs.ReduceAttrs(0x7f84a7c86398), [TensorType([4, 498, 128], float32)]), CallNode(Op(transpose), [CallNode(Op(transpose), [Var(toprobs.weight, ty=TensorType([2, 128], float32))], relay.attrs.TransposeAttrs(0x7f84a7cf0968), [TensorType([2, 128], float32)])], relay.attrs.TransposeAttrs(0x7f84a7ce6998), [TensorType([128, 2], float32)])], relay.attrs.DenseAttrs(0x7f84a7ca5e98), [TensorType([4, 128], float32), TensorType([2, 128], float32)]), Var(toprobs.bias, ty=TensorType([2], float32))], (nullptr), [TensorType([4, 2], float32), TensorType([2], float32)])], relay.attrs.SoftmaxAttrs(0x7f84a7ce6d78), [TensorType([4, 2], float32)]), [], (nullptr))})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_name=\"nn.batch_matmul\"\n",
    "# end_name=\"reshape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if target.device_name == \"vta\":\n",
    "        # Perform quantization in Relay\n",
    "        # Note: We set opt_level to 3 in order to fold batch norm\n",
    "        with tvm.transform.PassContext(opt_level=3):\n",
    "            with relay.quantize.qconfig(global_scale=8.0, skip_conv_layers=[0]):\n",
    "                mod = relay.quantize.quantize(mod, params=params)\n",
    "            # Perform graph packing and constant folding for VTA target\n",
    "            assert env.BLOCK_IN == env.BLOCK_OUT\n",
    "#             relay_prog = graph_pack(\n",
    "#                 mod[\"main\"],\n",
    "#                 env.BATCH,\n",
    "#                 env.BLOCK_OUT,\n",
    "#                 env.WGT_WIDTH,\n",
    "#                 start_name=start_name,\n",
    "#                 stop_name=end_name,\n",
    "\n",
    "#             )\n",
    "    else:\n",
    "        relay_prog = mod[\"main\"]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"-target\" is deprecated, use \"-mtriple\" instead.\n",
      "\"-target\" is deprecated, use \"-mtriple\" instead.\n",
      "\"-target\" is deprecated, use \"-mtriple\" instead.\n",
      "\"-target\" is deprecated, use \"-mtriple\" instead.\n",
      "\"-target\" is deprecated, use \"-mtriple\" instead.\n",
      "\"-target\" is deprecated, use \"-mtriple\" instead.\n",
      "Cannot find config for target=ext_dev -keys=vta,cpu -device=vta -model=sim_1x16_i8w8a32_15_15_18_17, workload=('dense_nopack.x86', ('TENSOR', (4, 128), 'float32'), ('TENSOR', (2, 128), 'float32'), None, 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "Cannot find config for target=ext_dev -keys=vta,cpu -device=vta -model=sim_1x16_i8w8a32_15_15_18_17, workload=('batch_matmul.x86', ('TENSOR', (4, 498, 512), 'float32'), ('TENSOR', (4, 128, 512), 'float32')). A fallback configuration is used, which may bring great performance regression.\n",
      "Cannot find config for target=ext_dev -keys=vta,cpu -device=vta -model=sim_1x16_i8w8a32_15_15_18_17, workload=('batch_matmul.x86', ('TENSOR', (4, 498, 128), 'float32'), ('TENSOR', (4, 512, 128), 'float32')). A fallback configuration is used, which may bring great performance regression.\n",
      "Cannot find config for target=ext_dev -keys=vta,cpu -device=vta -model=sim_1x16_i8w8a32_15_15_18_17, workload=('batch_matmul.x86', ('TENSOR', (4, 498, 128), 'float32'), ('TENSOR', (4, 128, 128), 'float32')). A fallback configuration is used, which may bring great performance regression.\n",
      "Cannot find config for target=ext_dev -keys=vta,cpu -device=vta -model=sim_1x16_i8w8a32_15_15_18_17, workload=('batch_matmul.x86', ('TENSOR', (4, 498, 498), 'float32'), ('TENSOR', (4, 128, 498), 'float32')). A fallback configuration is used, which may bring great performance regression.\n",
      "Cannot find config for target=ext_dev -keys=vta,cpu -device=vta -model=sim_1x16_i8w8a32_15_15_18_17, workload=('batch_matmul.x86', ('TENSOR', (4, 498, 128), 'float32'), ('TENSOR', (4, 498, 128), 'float32')). A fallback configuration is used, which may bring great performance regression.\n"
     ]
    }
   ],
   "source": [
    "# Compile Relay program with AlterOpLayout disabled\n",
    "if target.device_name != \"vta\":\n",
    "    with tvm.transform.PassContext(opt_level=3, disabled_pass={\"AlterOpLayout\"}):\n",
    "        graph, lib, params = relay.build(\n",
    "            mod, target=target, params=params, target_host=env.target_host\n",
    "        )\n",
    "else:\n",
    "    with vta.build_config(opt_level=3, disabled_pass={\"AlterOpLayout\"}):\n",
    "        lib = relay.build(mod, target=target, params=params, target_host=env.target_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Send the inference library over to the remote RPC server\n",
    "    from tvm.contrib import util\n",
    "    temp = util.tempdir()\n",
    "    lib.export_library(temp.relpath(\"graphlib.tar\"))\n",
    "    remote.upload(temp.relpath(\"graphlib.tar\"))\n",
    "    lib = remote.load_module(\"graphlib.tar\")\n",
    "\n",
    "    # Graph runtime\n",
    "    m = graph_runtime.GraphModule(lib[\"default\"](ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module(rpc, 7f84d0edc5b8)\n"
     ]
    }
   ],
   "source": [
    "print(lib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set inputs\n",
    "m.set_input(input_name, tvm.nd.array(input_data))\n",
    "# Execute\n",
    "m.run()\n",
    "# Get outputs\n",
    "tvm_output = m.get_output(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tvm.nd.NDArray shape=(4, 2), remote[0]:ext_dev(0)>\n",
       "array([[-0.7481267 , -0.6410334 ],\n",
       "       [-0.75686437, -0.63324785],\n",
       "       [-0.7323994 , -0.6553777 ],\n",
       "       [-0.76960146, -0.6221253 ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42453, 23662, 24983,  ...,  1561,  1524, 43533],\n",
       "        [36655, 11660, 41342,  ..., 30393,  6298, 16810],\n",
       "        [25529,  4065, 21512,  ..., 36405, 39718, 45385],\n",
       "        [47098,  4221, 36690,  ..., 47497, 22007,  7896]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_PATH='saved_model/input_data_vat.pt'\n",
    "torch.save(input_data, INPUT_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_output = tvm_output.asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_output = torch.from_numpy(np_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7481, -0.6410],\n",
       "        [-0.7569, -0.6332],\n",
       "        [-0.7324, -0.6554],\n",
       "        [-0.7696, -0.6221]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DATA_PATH='saved_model/output_data_vta.pt'\n",
    "torch.save(torch_output, OUTPUT_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate code (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mhost = tvm.build(mod, target=target)\n",
    "print(mod.astext(show_meta_data=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load Compiled Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tar error:\ntar: no files or directories specified\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-c5c14be8beec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mLIB_PATH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'saved_model/deploy_lib_vta.tar'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLIB_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Downloads/ml/sc/xilinx/Github/tvm/python/tvm/runtime/module.py\u001b[0m in \u001b[0;36mexport_library\u001b[0;34m(self, file_name, fcompile, addons, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"options\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mfcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/ml/sc/xilinx/Github/tvm/python/tvm/contrib/tar.py\u001b[0m in \u001b[0;36mtar\u001b[0;34m(output, files)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Tar error:\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tar error:\ntar: no files or directories specified\n"
     ]
    }
   ],
   "source": [
    "# save the graph, lib and params into separate files\n",
    "from tvm.contrib import util\n",
    "\n",
    "LIB_PATH='saved_model/deploy_lib_vta.tar'\n",
    "lib.export_library(LIB_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB_PATH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the module back.\n",
    "loaded_lib = tvm.runtime.load_module(LIB_PATH)\n",
    "\n",
    "\n",
    "m = graph_runtime.GraphModule(loaded_lib[\"default\"](ctx))\n",
    "# Set inputs\n",
    "m.set_input(input_name, tvm.nd.array(input_data))\n",
    "# Execute\n",
    "m.run()\n",
    "# Get outputs\n",
    "tvm_output = m.get_output(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
